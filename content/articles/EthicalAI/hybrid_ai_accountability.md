# The Architects of 'Because I Said So': When Humans Hide Behind Their Own Machines

*Or: How we learned to outsource our conscience and call it innovation*

---

**The Courtroom Where Accountability Went to Die**

Picture this: Wisconsin, 2013. Eric Loomis stands before a judge—but honestly? He might as well be facing a Magic 8-Ball wearing a judicial robe.

Here's what's happening. A human judge, trained in law and supposedly wisdom, is about to sentence a human defendant based on... wait for it... the recommendation of COMPAS¹. An algorithm that has mysteriously divined Loomis to be "high risk." 

The math behind this divine revelation? Classified. 
The methodology? Trade secret. 
The accountability? {*Gestures vaguely at the void*} - nowhere to be found.

This isn't science fiction. This is just another day at the office.

But here's what really went down in that courtroom: A bunch of humans built a system, deployed it, and then performed the most elaborate magic trick in legal history—making their own responsibility disappear behind a curtain of code.

Welcome to the age of algorithmic authority, where "the computer says so" has become the new "because I said so"/¹ except now nobody has to admit they said anything at all. Brilliant, really. If you're into that sort of thing.

---

**The Great Vanishing Act (Or: How Humans Learned to Disappear)**

Let's imagine the corporate boardroom where COMPAS was born. Somewhere, a product manager probably said, "What if we could make sentencing recommendations, but nobody could question our work?" And somewhere else, a lawyer perked up and said, "Intellectual property law has entered the chat."

The result? A system that operates like a judicial Ouija board—mystical, unquestionable, conveniently unaccountable.

Here's the thing about black boxes, though/\ they don't build themselves. They don't just materialize out of the digital ether with mysterious powers. Every algorithm is basically a fossil record of human choices. Someone decided what data to include. Someone chose what patterns to prioritize. Someone looked at the final product and said, "Yep, ship it."

The opacity goes beyond being a bug<< it's a feature. A feature of the business model, more than the technology.

We've created the perfect crime: injustice with no fingerprints. And somehow convinced ourselves this represents progress.

---

**Meet Your New Overlords (The Cast of Characters)**

**The Algorithmic Ventriloquists**: Companies like Northpointe (now Equivant) who built COMPAS. They've mastered the art of making their biases speak through mathematical puppets. "It wasn't us," they say, with the practiced innocence of someone who's never met a liability they couldn't deflect. "It was the algorithm." As if algorithms vote themselves into existence.

**The Responsibility Refugees**: Judges, HR managers, loan officers—people who once made decisions and now defer to systems like they're consulting an oracle. They've discovered something beautiful: you can't be blamed for bias if you outsource it to math. Having a scapegoat that runs on electricity and never talks back? [*Immaculate conception of responsibility avoidance*]

**The Data Alchemists**: Those who transform historical injustice into "training data" and somehow expect it to produce fair outcomes. They feed the algorithm decades of biased decisions and act genuinely surprised when it perpetuates bias. Teaching a parrot to speak by playing it recordings of family arguments, then wondering why it's so rude at dinner parties? ^Same energy^

**The Silence Merchants**: The legal and technical establishment that has decided opacity is not just acceptable but somehow... sophisticated? They've convinced us that incomprehensible equals intelligent. That mystery equals mastery.

---

**The Economics of Evasion (Or: Why This Nonsense Pays)**

Why do we tolerate algorithmic authoritarianism? Follow the money and the comfort—they usually travel together.

For vendors, mystery equals market advantage. If everyone could see how COMPAS works, everyone could build COMPAS. Transparency is bad for business. Duh.

For institutions, algorithms are the ultimate liability shield. When the system makes a spectacularly bad call, nobody's job is on the line. Corruption? ~No no~ (innovation! The exclamation point is mandatory in all press releases.)

For individuals in power, these systems offer something precious: the ability to make consequential decisions without the weight of moral responsibility. They can sleep at night because technically, the computer did it. Their conscience is clear—or at least, comfortably cloudy.

We've created a market for moral outsourcing, and business is absolutely booming.

---

**What We've Lost (And What We Must Reclaim Before It's Too Late)**

In the rush to algorithmize authority, we've forgotten something kind of fundamental: power without accountability goes beyond being progress|\ regression to divine right, now with better marketing and a sleeker interface.

When Eric Loomis was sentenced, something died in that courtroom. Justice? [Too dramatic.] Something more basic: the expectation that power should explain itself. The idea that authority comes with the obligation to show your work.

Let's do a quick thought experiment. Imagine if we applied this logic elsewhere:

*Doctor*: "I can't tell you why I prescribed this medication. It's proprietary."
*Engineer*: "I can't show you the bridge blueprints. Trade secret."
*Teacher*: "I can't explain why your child failed. The algorithm is confidential."

We'd lose our minds. Rightfully so. So why /seriously, why\ do we accept it in criminal justice? In hiring? In loan approvals?

---

**The Path Forward: Algorithms with Consequences**

Here's a genuinely radical idea: What if algorithms came with accountability built in?

What if COMPAS had looked at Eric Loomis's case and said, "Nope. Insufficient data quality. Bias detected. Please consult actual human judgment"? Ethical guardrails instead of just legal ones? ||Revolutionary concept||

What if every algorithmic system came with a warning label—you know, like cigarettes, but for justice:

> **CAUTION: This system reflects the biases of its training data and creators. It may perpetuate historical injustices. It cannot replace human moral reasoning. Side effects may include algorithmic discrimination, false confidence, and the gradual erosion of human accountability. Use with extreme caution and continuous oversight.**

What if we actually demanded:

- **Algorithmic Transparency**: No secret sauce in public decisions. Period.
- **Human Override**: Algorithms advise, humans decide (and take responsibility for those decisions)
- **Bias Auditing**: Regular checkups for systemic unfairness, like health inspections but for justice
- **Right to Explanation**: If a system affects your life, you deserve to understand why

^These aren't impossible standards^. They're basic expectations we've somehow forgotten to enforce.

---

**A Different Ending (The One We Deserved)**

Let me tell you how Eric Loomis's story should have ended.

The judge looks at the COMPAS report. Really looks at it. Then she sets it aside—not dramatically, just... deliberately.

She says, "I am the one making this decision. I am the one who will be accountable for it. And I will base it on evidence I can examine, law I can cite, and human judgment I can explain (rather than on a system that refuses to show its work.)"

Something other than just a different ending for Eric Loomis? A different world. One where authority comes with explanation, where power requires accountability, where "because the algorithm said so" gets filed under "unacceptable answers."

---

**Conclusion: The Authority We Actually Deserve**

We're standing at a crossroads—though let's be honest, we've been standing here for a while now, checking our phones and occasionally glancing up at the street signs.

We can keep going down this path where power hides behind algorithms and responsibility evaporates into the cloud like digital vapor. Or we can demand something different. Something better.

Algorithms ~aren't~ our new gods (despite what Silicon Valley's marketing department wants us to believe). They're tools. Sophisticated tools, sure, but tools nonetheless. And tools should serve human values, not replace human judgment.

The question goes beyond whether AI will be part of our future (it will be, obviously). The question is whether we'll let it be used to obscure human responsibility or to enhance human wisdom. Whether we'll use it to avoid difficult decisions or to make better ones.

Eric Loomis was sentenced by humans who built a system and then pretended they hadn't. The algorithm didn't fail (the humans behind it did). They failed to take responsibility. They failed to ensure fairness. They failed to remember that justice goes beyond efficiency/\ it's about accountability.

If we want algorithmic justice, we need human accountability. If we want better systems, we need braver people building and deploying them.

Show your work. Take responsibility. Stand behind your systems (or don't deploy them).

And maybe /just maybe\ we can build a future where "because I said so" gets replaced by something revolutionary: "Here's why, and here's how you can check."

---

¹ COMPAS stands for Correctional Offender Management Profiling for Alternative Sanctions. For detailed analysis of the Loomis case, see State v. Loomis, 881 N.W.2d 749 (Wis. 2016).

² Yes, creative punctuation choices ahead. Consider this gentle resistance to the current witch trials against em dashes and their ilk. Some of us still believe in the full range of human expression, algorithmic assistance or otherwise.

The algorithms are listening. The question is: are we brave enough to speak up?